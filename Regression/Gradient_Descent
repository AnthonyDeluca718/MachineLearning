=== Understanding Stochastic Gradient Descent:\

1) Simples case:

MINIMIZE (1/2)[(x - y)^2 + (x + y)^2] (y > 0)
x -> x + e(y-x) = (1-e)x + ey
(1-e)x + ey -> (1-e)((1-e)x + ey) + e(-y)
  = (x-ex + ey) - e(x-ex + ey) - ey)
  = x-ex + ey - ex + e^2x - e^2y - ey
  = x - 2ex + ey -ey
  = (1-2e)x

So we trivially get convergence


2) Minimizing (1/2)*SUM (x - yi)^2
x -> x + e(yi - x1) = (1-e)x + e*yi
assume xk-1 = (1-[k-1]e)x0 + e(y1 + ... + yk-1)
xk = (1-e)xk-1 + e*yk = (1-e)(1-[k-1]e)x0 + (1-e)e(y1 + ... + yk-``) + e*yk
  = (1-ke)x0 + e(y1 + ... + yk-1) + e*yk
  = (1-ke)x0 + e(y1 + ... + yk)

so xN = (1-ne)x0 + e(y1 + ... yk) = (1-ne)x0 + ne(AVG(y))

changing variables we can take AVG(y) = 0

so xN = (1-ne)x0 -> this process converges to 0


3) Batch/Random Gradient Descent

X will move toward the average value of the sample/batch. So large batches/n-count/samples of batches will move x toward the true average value.


4) Why can we think in terms of 1-D case?
